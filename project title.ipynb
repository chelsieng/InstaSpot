{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423060ba",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5042972",
   "metadata": {},
   "source": [
    "-description-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac166938",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1db7c1",
   "metadata": {},
   "source": [
    "1. Importing modules\n",
    "\n",
    "2. Data processing\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab538325",
   "metadata": {},
   "source": [
    "## 1. Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4f9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852dbd75",
   "metadata": {},
   "source": [
    "## 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7bb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c28892",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2681551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4210\n",
      "4210\n",
      "wake.up.matt\n"
     ]
    }
   ],
   "source": [
    "spark = init_spark()\n",
    "\n",
    "lines = spark.sparkContext.textFile('data/influencers.txt')\n",
    "\n",
    "# get category and username index\n",
    "headers = lines.take(2)\n",
    "header = headers[0]\n",
    "category_index = header.split(\"\\t\").index(\"Category\")\n",
    "username_index = header.split(\"\\t\").index(\"Username\")\n",
    "\n",
    "# filter travel influencers\n",
    "lines = lines.filter(lambda line: line not in headers)\n",
    "lines = lines.map(lambda line: line.split(\"\\t\"))\n",
    "travel_influencers = lines.filter(lambda line: line[category_index] == 'travel')\n",
    "# get all travel influencers IG username\n",
    "travel_usernames = travel_influencers.map(lambda line: line[username_index])\n",
    "\n",
    "print(travel_influencers.count())\n",
    "print(travel_usernames.count())\n",
    "print(travel_usernames.collect()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e2fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zip/', 'zip/test1.txt', 'zip/test2.txt', 'zip/test3.txt']\n",
      "zip/test1.txt\n",
      "zip/test2.txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: reduce time complexity\n",
    "with ZipFile('data/post-metadata/fix.zip', 'r') as zipObject:\n",
    "    names = zipObject.namelist()\n",
    "    for file_name in names:\n",
    "        if any(i in file_name for i in travel_usernames.collect()):\n",
    "            # Extract a travel influencers post metadata from zip\n",
    "            zipObject.extract(file_name, 'data/post-metadata')\n",
    "            print('All the python files are extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebea59-084c-49f4-9046-5dd8cdbb8706",
   "metadata": {},
   "source": [
    "# Data Processing, Reading Data Files, and Convert them to RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45378b25-66d6-4584-9ce4-df10a25159a9",
   "metadata": {},
   "source": [
    "### All the following cells need to be executed unless the cell is specify as OPTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9576b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder where the .info files are located. \n",
    "# MODIFY THE SOURCE DIRECTORY ACCORDINGLY\n",
    "source_directory = '../data_instagram/deleteme/*.info'\n",
    "source_directory = './test_01/*.info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35ad4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess functions need to extract required fields\n",
    "def extract_counts(row, field):\n",
    "    if field not in row:\n",
    "        return 0\n",
    "    if row[field] is None:\n",
    "        return 0\n",
    "    if 'count' not in row[field] or row[field]['count'] is None:\n",
    "        return 0\n",
    "    return row[field]['count']\n",
    "\n",
    "def likes(row):\n",
    "    return extract_counts(row, 'edge_media_preview_like')\n",
    "\n",
    "def comments_count(row):\n",
    "    return extract_counts(row, 'edge_media_to_parent_comment')\n",
    "\n",
    "def extract_nodes_from_edges(row, field, secondary_fields):\n",
    "    result = []\n",
    "    if field not in row or row[field] is None \\\n",
    "    or 'edges' not in row[field] or row[field]['edges'] is None:\n",
    "        return []\n",
    "\n",
    "    for edge in row[field]['edges']:\n",
    "        if 'node' in edge and edge['node']:\n",
    "            no_error = True\n",
    "            temp = edge['node']\n",
    "            for f in secondary_fields:\n",
    "                if f in temp and temp[f]:\n",
    "                    temp = temp[f]\n",
    "                else:\n",
    "                    no_error = False\n",
    "                    \n",
    "            if not no_error:\n",
    "                result.append(temp)\n",
    " \n",
    "    return result\n",
    "    \n",
    "def extract_tagged_users_id(row):\n",
    "    #edge_media_to_tagged_user.edges.[i].node.user.id\n",
    "    return extract_nodes_from_edges(row, 'edge_media_to_tagged_user', ['user', 'id'])\n",
    "    '''\n",
    "    result = []\n",
    "    if 'edge_media_to_tagged_user' not in row or row['edge_media_to_tagged_user'] is None \\\n",
    "    or 'edges' not in row['edge_media_to_tagged_user'] or row['edge_media_to_tagged_user']['edges'] is None:\n",
    "        return []\n",
    "\n",
    "    for edge in row['edge_media_to_tagged_user']['edges']:\n",
    "        if 'node' in edge and edge['node'] and 'user' in edge['node'] and edge['node']['user'] and 'id' in edge['node']['user']:\n",
    "            result.append(edge['node']['user']['id'])\n",
    "    return result\n",
    "    '''\n",
    "\n",
    "def extract_commenters_id(row):\n",
    "    #edge_media_to_parent_comment.edges.[i].node.owner.id\n",
    "    return extract_nodes_from_edges(row, 'edge_media_to_parent_comment', ['owner', 'id'])\n",
    "    '''\n",
    "    result = []\n",
    "    if 'edge_media_to_parent_comment' not in row or row['edge_media_to_parent_comment'] is None \\\n",
    "    or 'edges' not in row['edge_media_to_parent_comment'] or row['edge_media_to_parent_comment']['edges'] is None:\n",
    "        return []\n",
    "\n",
    "    for edge in row['edge_media_to_parent_comment']['edges']:\n",
    "        if 'node' in edge and edge['node'] and 'owner' in edge['node'] and edge['node']['owner'] and 'id' in edge['node']['owner']:\n",
    "            result.append(edge['node']['owner']['id'])\n",
    "    return result\n",
    "    '''\n",
    "\n",
    "def extract_text_from_caption(row):\n",
    "    #edge_media_to_caption.edges.node.text\n",
    "    result = []\n",
    "    if 'edge_media_to_caption' not in row or row['edge_media_to_caption'] is None \\\n",
    "    or 'edges' not in row['edge_media_to_caption'] or row['edge_media_to_caption']['edges'] is None:\n",
    "        return []\n",
    "    \n",
    "    for edge in row['edge_media_to_caption']['edges']:\n",
    "        if 'node' in edge and edge['node'] and 'text' in edge['node']:\n",
    "            result.append(edge['node']['text'])\n",
    "    return result\n",
    "\n",
    "def extract_location(row):\n",
    "    result = {\n",
    "        'location_name': '',\n",
    "        'location_id': ''\n",
    "    }\n",
    "    if 'location' in row and row['location']:\n",
    "        if 'name' in row['location']:\n",
    "            result['location_name'] = row['location']['name']\n",
    "        if 'id' in row['location']:\n",
    "            result['location_id']   = row['location']['id']\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def extract_post_owner_username(row):\n",
    "    if 'owner' not in row or row['owner'] is None:\n",
    "        return ''\n",
    "    \n",
    "    if 'username' not in row['owner'] or row['owner']['username'] is None:\n",
    "        return ''\n",
    "\n",
    "    return row['owner']['username']\n",
    "    \n",
    "#This function return an RDD where each row is a json doc\n",
    "def create_post_as_json(row):\n",
    "    post_id = row['id'] or ''\n",
    "    location = extract_location(row)\n",
    "    owner_username = extract_post_owner_username(row)\n",
    "    texts = extract_text_from_caption(row)\n",
    "    count_likes = likes(row)\n",
    "    tagged_users_id = extract_tagged_users_id(row)\n",
    "    commenters_id = extract_commenters_id(row)    \n",
    "    \n",
    "    return {\n",
    "        'post_id': post_id,\n",
    "        'location_name' : location['location_name'],\n",
    "        'location_id' : location['location_id'],\n",
    "        'count_likes': count_likes,\n",
    "        'owner_username': owner_username,\n",
    "        'captions': texts,\n",
    "        'tagged_users_id': tagged_users_id,\n",
    "        'commenters_id': commenters_id\n",
    "    }\n",
    "\n",
    "#when exporting the data to CSV, it doesn't allow arrays, so the they needs to be converted into strings\n",
    "def flatten_json_lists(row):\n",
    "    row['captions'] = '. '.join(row['captions'])\n",
    "    row['tagged_users_id'] =  ', '.join(row['tagged_users_id'])\n",
    "    row['commenters_id'] =  ', '.join(row['commenters_id'])\n",
    "    return row\n",
    "\n",
    "#convert a json doc into tuples\n",
    "def convert_json_to_tuple(row):\n",
    "    post_id = row['post_id']\n",
    "    location_name = row['location_name']\n",
    "    location_id = row['location_id']\n",
    "    count_likes = row['count_likes']\n",
    "    owner_username = row['owner_username']\n",
    "    texts = row['captions']\n",
    "    tagged_users_id = row['tagged_users_id']\n",
    "    commenters_id = row['commenters_id']\n",
    "    return (post_id, location_name, location_id, \n",
    "            count_likes, owner_username, texts, tagged_users_id, commenters_id)\n",
    "    \n",
    "\n",
    "def remove_carry_returns(row):\n",
    "    row['captions'] = row['captions'].replace('\\r', '').replace('\\n', ' ')\n",
    "    return row\n",
    "    \n",
    "#same as the previous function, but the return type is a tuple (NOT USED)\n",
    "def create_post_as_tuple(row):\n",
    "    post_id = row['id'] or ''\n",
    "    location = extract_location(row)\n",
    "    owner_username = extract_post_owner_username(row)\n",
    "    texts = '\\n'.join(extract_text_from_caption(row))\n",
    "    count_likes = likes(row)\n",
    "    tagged_users_id = ', '.join(extract_tagged_users_id(row))\n",
    "    commenters_id = ', '.join(extract_commenters_id(row))\n",
    "    return (post_id, location['location_name'], location['location_id'], \n",
    "            count_likes, owner_username, texts, tagged_users_id, commenters_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d29060b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from source directory\n",
    "df = spark.read.json(source_directory)\n",
    "rdd =  df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b453672-673c-42b9-9c50-0a65a8f934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform data to the needed format\n",
    "clean_data = rdd.map(lambda r: create_post_as_json(r)).\\\n",
    "    map(lambda r: flatten_json_lists(r)).\\\n",
    "    map(lambda r: remove_carry_returns(r)).\\\n",
    "    map(lambda r: convert_json_to_tuple(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "722e5f90-73c6-4b49-afae-a17806cd8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL, run this if you want to see the data printed\n",
    "df = clean_data.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "718c2f4a-d866-4dd9-ae81-7e58e631176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------+-----------+--------------------+--------------------+---------------+-------------+\n",
      "|            post_id|       location_name|     location_id|count_likes|      owner_username|            captions|tagged_users_id|commenters_id|\n",
      "+-------------------+--------------------+----------------+-----------+--------------------+--------------------+---------------+-------------+\n",
      "|1991526353976830561| Ristorante  Il Moro|       243270193|       1269|     abakumovanastia|[ P E S C E ] –ö–∞–∫...|               |             |\n",
      "|1825852844165346062|Half Moon Bay, Ca...|       213762097|       4851|        abekislevitz|This month marks ...|               |             |\n",
      "|1749451773011353261|     Capitoline Hill|       221222985|       1330|     abakumovanastia|[ T E R R A Z Z A...|               |             |\n",
      "|1668226920279477602|Terrazza Della Ri...| 153841095220974|       1297|     abakumovanastia|[ L E G G E N D A...|               |             |\n",
      "|1778172779343163351|        Lecce, Italy|       213052715|       1425|     abakumovanastia|[ E U R O V I S I...|               |             |\n",
      "|1939074027026654368|           L'Euclide|      1020054854|        836|     abakumovanastia|[ V I N  C H A U ...|               |             |\n",
      "|1784976583694988665| Cottonwood, Arizona|       235015840|       1725|a.girl.and.her.co...|We are the same b...|               |             |\n",
      "|1929192123523858752|  Hotel Locarno Roma|         4904980|        753|     abakumovanastia|[ N E W S ] –ù–æ–≤–æ—Å...|               |             |\n",
      "|1878171894453580298|  Terrazza Borromini|1882470335368611|       1171|     abakumovanastia|[ G E I S U I T I...|               |             |\n",
      "|1724830613116324156|      Palazzo Mattei|       504532824|       1193|     abakumovanastia|[ R O M A ~  A M ...|               |             |\n",
      "|1782822799799422748|Fontana Di Trevi-...|      1018776195|       1561|     abakumovanastia|[ T R E V I ] –ú–Ω–µ...|               |             |\n",
      "|1723076871333404468|Colosseo, Roma, I...|       235445983|       5051|     abakumovanastia|[ C O L O S S E O...|               |             |\n",
      "|1755113644135187764|     Porto, Portugal|        24960421|        572|         abbyjobowes|Spent the night i...|               |             |\n",
      "|1683519692059516815|        Machu Picchu|       228001889|        665|         abbyjobowes|Have some fun thi...|               |             |\n",
      "|1907452939641726078|  Terrazza Borromini|1882470335368611|        973|     abakumovanastia|[ P E R C O R S O...|               |             |\n",
      "|1968595577186124843|              Holbox| 272650963244200|        787|         2traveldads|Cartwheels and ha...|               |             |\n",
      "|1950960789093200256|             Abruzzo|       420438107|        998|     abakumovanastia|[ G R A N  S A S ...|               |             |\n",
      "|2000889721153635401|Bryce Canyon Nati...|      1015618315|       1057|         2traveldads|Know how we're al...|               |             |\n",
      "|1901594885670368459|The Metropolitan ...|          378453|       1271|          47giraffes|the Met was a dre...|               |             |\n",
      "|1865628032589785314|The Middle of Now...|2053495638241578|       2535|a.girl.and.her.co...|üåªI'm in a town s...|               |             |\n",
      "+-------------------+--------------------+----------------+-----------+--------------------+--------------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OPTIONAL, run this if want to see data printed. You need to run the previous cell  for this one to work\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22a88f18-76b0-4f6c-bf9d-5e12ebeef474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = ['post_id', 'location_name', 'location_id', 'count_likes', 'owner_username', 'captions', 'tagged_users_id', 'commenters_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "78e73c7f-f5ed-4bc9-93ae-61f298d9517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data into csv files\n",
    "#rdd.map(lambda r: create_post_as_tuple(r)).toDF(schema).write.format(\"com.databricks.spark.csv\").save(\"csv_formated_data\", header=\"true\")\n",
    "\n",
    "clean_data.toDF(schema).write.format(\"com.databricks.spark.csv\").save(\"csv_formated_data\", header=\"true\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
