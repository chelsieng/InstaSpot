{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423060ba",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5042972",
   "metadata": {},
   "source": [
    "-description-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac166938",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1db7c1",
   "metadata": {},
   "source": [
    "1. Importing modules\n",
    "\n",
    "2. Data processing\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab538325",
   "metadata": {},
   "source": [
    "## 1. Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4f9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852dbd75",
   "metadata": {},
   "source": [
    "## 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7bb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c28892",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2681551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4210\n",
      "4210\n",
      "wake.up.matt\n"
     ]
    }
   ],
   "source": [
    "spark = init_spark()\n",
    "\n",
    "lines = spark.sparkContext.textFile('data/influencers.txt')\n",
    "\n",
    "# get category and username index\n",
    "headers = lines.take(2)\n",
    "header = headers[0]\n",
    "category_index = header.split(\"\\t\").index(\"Category\")\n",
    "username_index = header.split(\"\\t\").index(\"Username\")\n",
    "\n",
    "# filter travel influencers\n",
    "lines = lines.filter(lambda line: line not in headers)\n",
    "lines = lines.map(lambda line: line.split(\"\\t\"))\n",
    "travel_influencers = lines.filter(lambda line: line[category_index] == 'travel')\n",
    "# get all travel influencers IG username\n",
    "travel_usernames = travel_influencers.map(lambda line: line[username_index])\n",
    "\n",
    "print(travel_influencers.count())\n",
    "print(travel_usernames.count())\n",
    "print(travel_usernames.collect()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e2fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zip/', 'zip/test1.txt', 'zip/test2.txt', 'zip/test3.txt']\n",
      "zip/test1.txt\n",
      "zip/test2.txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: reduce time complexity\n",
    "with ZipFile('data/post-metadata/fix.zip', 'r') as zipObject:\n",
    "    names = zipObject.namelist()\n",
    "    for file_name in names:\n",
    "        if any(i in file_name for i in travel_usernames.collect()):\n",
    "            # Extract a travel influencers post metadata from zip\n",
    "            zipObject.extract(file_name, 'data/post-metadata')\n",
    "            print('All the python files are extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebea59-084c-49f4-9046-5dd8cdbb8706",
   "metadata": {},
   "source": [
    "# Data Processing, Reading Data Files, and Convert them to RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45378b25-66d6-4584-9ce4-df10a25159a9",
   "metadata": {},
   "source": [
    "### All the following cells need to be executed unless the cell is specify as OPTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9576b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder where the .info files are located. \n",
    "# MODIFY THE SOURCE DIRECTORY ACCORDINGLY\n",
    "source_directory = '../data_instagram/deleteme/*.info'\n",
    "#source_directory = './test_01/*.info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35ad4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess functions need to extract required fields\n",
    "def extract_counts(row, field):\n",
    "    if field not in row:\n",
    "        return 0\n",
    "    if row[field] is None:\n",
    "        return 0\n",
    "    if 'count' not in row[field] or row[field]['count'] is None:\n",
    "        return 0\n",
    "    return row[field]['count']\n",
    "\n",
    "def likes(row):\n",
    "    return extract_counts(row, 'edge_media_preview_like')\n",
    "\n",
    "def comments_count(row):\n",
    "    return extract_counts(row, 'edge_media_to_parent_comment')\n",
    "\n",
    "def extract_nodes_from_edges(row, field, secondary_fields):\n",
    "    result = []\n",
    "    if field not in row or row[field] is None \\\n",
    "    or 'edges' not in row[field] or row[field]['edges'] is None:\n",
    "        return []\n",
    "\n",
    "    for edge in row[field]['edges']:\n",
    "        if 'node' in edge and edge['node']:\n",
    "            no_error = True\n",
    "            temp = edge['node']\n",
    "            for f in secondary_fields:\n",
    "                if f in temp and temp[f]:\n",
    "                    temp = temp[f]\n",
    "                else:\n",
    "                    no_error = False\n",
    "                    \n",
    "            if no_error:\n",
    "                result.append(temp)\n",
    " \n",
    "    return result\n",
    "    \n",
    "def extract_tagged_users_id(row):\n",
    "    #edge_media_to_tagged_user.edges.[i].node.user.id\n",
    "    return extract_nodes_from_edges(row, 'edge_media_to_tagged_user', ['user', 'id'])\n",
    "\n",
    "def extract_commenters_id(row):\n",
    "    #edge_media_to_parent_comment.edges.[i].node.owner.id\n",
    "    return extract_nodes_from_edges(row, 'edge_media_to_parent_comment', ['owner', 'id'])\n",
    "\n",
    "def extract_text_from_caption(row):\n",
    "    #edge_media_to_caption.edges.node.text\n",
    "    result = []\n",
    "    if 'edge_media_to_caption' not in row or row['edge_media_to_caption'] is None \\\n",
    "    or 'edges' not in row['edge_media_to_caption'] or row['edge_media_to_caption']['edges'] is None:\n",
    "        return []\n",
    "    \n",
    "    for edge in row['edge_media_to_caption']['edges']:\n",
    "        if 'node' in edge and edge['node'] and 'text' in edge['node']:\n",
    "            result.append(edge['node']['text'])\n",
    "    return result\n",
    "\n",
    "def extract_location(row):\n",
    "    result = {\n",
    "        'location_name': '',\n",
    "        'location_id': ''\n",
    "    }\n",
    "    if 'location' in row and row['location']:\n",
    "        if 'name' in row['location']:\n",
    "            result['location_name'] = row['location']['name']\n",
    "        if 'id' in row['location']:\n",
    "            result['location_id']   = row['location']['id']\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def extract_post_owner_username(row):\n",
    "    if 'owner' not in row or row['owner'] is None:\n",
    "        return ''\n",
    "    \n",
    "    if 'username' not in row['owner'] or row['owner']['username'] is None:\n",
    "        return ''\n",
    "\n",
    "    return row['owner']['username']\n",
    "    \n",
    "#This function return an RDD where each row is a json doc\n",
    "def create_post_as_json(row):\n",
    "    post_id = row['id'] or '' #post id\n",
    "    location = extract_location(row)\n",
    "    owner_username = extract_post_owner_username(row)\n",
    "    texts = extract_text_from_caption(row)\n",
    "    count_likes = likes(row)\n",
    "    tagged_users_id = extract_tagged_users_id(row) #tagged user's id\n",
    "    commenters_id = extract_commenters_id(row) #commenter user's id\n",
    "    comment_count = comments_count(row)\n",
    "    \n",
    "    return {\n",
    "        'post_id': post_id,\n",
    "        'location_name' : location['location_name'],\n",
    "        'location_id' : location['location_id'],\n",
    "        'count_likes': count_likes,\n",
    "        'owner_username': owner_username,\n",
    "        'captions': texts,\n",
    "        'tagged_users_id': tagged_users_id,\n",
    "        'commenters_id': commenters_id,\n",
    "        'accessibility_caption': row['accessibility_caption'],\n",
    "        'owner_id':row['owner']['id'],\n",
    "        'comments_count': comment_count\n",
    "    }\n",
    "\n",
    "#when exporting the data to CSV, it doesn't allow arrays, so the they needs to be converted into strings\n",
    "def flatten_json_lists(row):\n",
    "    row['captions'] = '. '.join(row['captions'])\n",
    "    row['tagged_users_id'] =  ', '.join(row['tagged_users_id'])\n",
    "    row['commenters_id'] =  ', '.join(row['commenters_id'])\n",
    "    return row\n",
    "\n",
    "#convert a json doc into tuples\n",
    "def convert_json_to_tuple(row):\n",
    "    post_id = row['post_id']\n",
    "    location_name = row['location_name']\n",
    "    location_id = row['location_id']\n",
    "    count_likes = row['count_likes']\n",
    "    owner_username = row['owner_username']\n",
    "    texts = row['captions']\n",
    "    tagged_users_id = row['tagged_users_id']\n",
    "    commenters_id = row['commenters_id']\n",
    "    accessibility_caption = row['accessibility_caption']\n",
    "    owner_id = row['owner_id']\n",
    "    comment_count = row['comments_count']\n",
    "    return (post_id, location_name, location_id, \n",
    "            count_likes, owner_username, texts, tagged_users_id, \n",
    "            commenters_id, accessibility_caption, owner_id, comment_count)\n",
    "    \n",
    "    \n",
    "\n",
    "def remove_carry_returns(row):\n",
    "    row['captions'] = row['captions'].replace('\\r', '').replace('\\n', ' ')\n",
    "    return row\n",
    "    \n",
    "#same as the previous function, but the return type is a tuple (NOT USED)\n",
    "def create_post_as_tuple(row):\n",
    "    post_id = row['id'] or ''\n",
    "    location = extract_location(row)\n",
    "    owner_username = extract_post_owner_username(row)\n",
    "    texts = '\\n'.join(extract_text_from_caption(row))\n",
    "    count_likes = likes(row)\n",
    "    tagged_users_id = ', '.join(extract_tagged_users_id(row))\n",
    "    commenters_id = ', '.join(extract_commenters_id(row))\n",
    "    return (post_id, location['location_name'], location['location_id'], \n",
    "            count_likes, owner_username, texts, tagged_users_id, commenters_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d29060b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from source directory\n",
    "df = spark.read.json(source_directory)\n",
    "rdd =  df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b453672-673c-42b9-9c50-0a65a8f934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform data to the needed format\n",
    "clean_data = rdd.map(lambda r: create_post_as_json(r)).\\\n",
    "    map(lambda r: flatten_json_lists(r)).\\\n",
    "    map(lambda r: remove_carry_returns(r)).\\\n",
    "    map(lambda r: convert_json_to_tuple(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e77d36a8-51c1-4a69-bd53-27398c43da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = ['post_id', 'location_name', 'location_id', \n",
    "          'count_likes', 'owner_username', 'captions', \n",
    "          'tagged_users_id', 'commenters_id', 'accessibility_caption', \n",
    "          'owner_id', 'comments_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "722e5f90-73c6-4b49-afae-a17806cd8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL, run this if you want to see the data printed\n",
    "df = clean_data.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "718c2f4a-d866-4dd9-ae81-7e58e631176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+-----------+--------------------+--------------------+--------------------+--------------------+---------------------+----------+--------------+\n",
      "|            post_id|       location_name|location_id|count_likes|      owner_username|            captions|     tagged_users_id|       commenters_id|accessibility_caption|  owner_id|comments_count|\n",
      "+-------------------+--------------------+-----------+-----------+--------------------+--------------------+--------------------+--------------------+---------------------+----------+--------------+\n",
      "|1466156969374840754|                    |           |       1802|             2themtn|I am so damn prou...|7142802, 21091305...|1591098326, 43463...| Image may contain...| 175920953|            81|\n",
      "|1465379418746479821|Yosemite National...|  886068655|       1844|             2themtn|A not so clear tu...|29162575, 1744351...|286284853, 372027...| No photo descript...| 175920953|            73|\n",
      "|1097368352458384583|                    |           |        125|14th_avenue_photo...|Few more hours le...|                    |            25456099| No photo descript...|1983715616|             1|\n",
      "|1082843722192364979|                    |           |         64|14th_avenue_photo...|•• Birds •• #bird...|                    |                    | No photo descript...|1983715616|             0|\n",
      "+-------------------+--------------------+-----------+-----------+--------------------+--------------------+--------------------+--------------------+---------------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OPTIONAL, run this if want to see data printed. You need to run the previous cell  for this one to work\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "78e73c7f-f5ed-4bc9-93ae-61f298d9517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data into csv files\n",
    "#rdd.map(lambda r: create_post_as_tuple(r)).toDF(schema).write.format(\"com.databricks.spark.csv\").save(\"csv_formated_data\", header=\"true\")\n",
    "\n",
    "clean_data.toDF(schema).write.format(\"com.databricks.spark.csv\").save(\"csv_formated_data\", header=\"true\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
